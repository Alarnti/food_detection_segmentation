{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781571ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import typing\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "import errno\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import FoodDataset\n",
    "from src.vis import read_image, show_image_coco\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00571cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "TRAIN_IMAGES_PATH = 'data/public_training_set_release_2.0/images/'\n",
    "TRAIN_LABELS = 'data/public_training_set_release_2.0/annotations.json'\n",
    "\n",
    "VAL_IMAGES_PATH = 'data/public_validation_set_2.0/images/'\n",
    "VAL_LABELS = 'data/public_validation_set_2.0/annotations.json'\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "MODEL_SAVE_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e595c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ind_img(ds: COCO, ind: int, ims_path: str) -> None:\n",
    "    img_ids = ds.getImgIds()\n",
    "    \n",
    "    return show_image_coco(img_ids[ind], ims_path, ds, True, True)\n",
    "\n",
    "def show_random_img(ds: COCO, ims_path: str) -> None:\n",
    "    img_ids = ds.getImgIds()\n",
    "    rand_ind = np.random.randint(len(img_ids))\n",
    "    \n",
    "    return show_image_coco(img_ids[rand_ind], ims_path, ds, True, True)\n",
    "\n",
    "ds_coco = COCO(TRAIN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa199cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_obj = ds_coco.loadAnns(ds_coco.getAnnIds(131094))\n",
    "anns_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ds_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b03a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_coco.getCatIds()\n",
    "{coco_id: ind for ind, coco_id  in enumerate(sorted(ds_coco.getCatIds()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random_img(ds_coco, TRAIN_IMAGES_PATH)\n",
    "show_ind_img(ds_coco, 1, TRAIN_IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_ds = FoodDataset(TRAIN_IMAGES_PATH, TRAIN_LABELS)\n",
    "\n",
    "torch_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1202ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ind_img(ds_coco,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_ds[8][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mask_bb(torch_ds, 22130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47d79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2afd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e82aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1a7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4b4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d35324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(torch_ds):\n",
    "    \n",
    "\n",
    "def test_mAP(ds):\n",
    "    mAP = MeanAveragePrecision(\n",
    "                box_format='xyxy',\n",
    "                iou_thresholds=None,\n",
    "                rec_thresholds=[1, 10, 100],\n",
    "                class_metrics=False,\n",
    "                )\n",
    "\n",
    "    metr.update(trgt, trgt)\n",
    "    pprint(metr.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30708b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04ca93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52115149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40972650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model_ft.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac604b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model_ft.transform(torch.unsqueeze(train_ds[1000][0], dim=0))[0]\n",
    "z.image_sizes# train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[1000][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grcnn = torchvision.models.detection.transform.GeneralizedRCNNTransform(min_size=700, max_size=700, image_mean=[0.485], image_std=[0.229])\n",
    "model_ft.transform = grcnn\n",
    "\n",
    "for el in train_ds:\n",
    "    z = model_ft.transform(torch.unsqueeze(el[0], dim=0))[0]\n",
    "    print(z.image_sizes)# train_ds[0][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2846eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.cpu().eval()\n",
    "\n",
    "raw_val = [torch_ds[i][0] for i in range(0,10)]\n",
    "trgt = [torch_ds[i][1] for i in range(0,10)]\n",
    "im_val = [torch.mul(255, raw_val[i][0]) for i in range(0,10)]\n",
    "\n",
    "res = model_ft(raw_val)\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metr = MeanAveragePrecision(\n",
    "                box_format='xyxy',\n",
    "                iou_thresholds=None,\n",
    "                rec_thresholds=[1, 10, 100],\n",
    "                class_metrics=False,\n",
    "                )\n",
    "\n",
    "metr.update(trgt, trgt)\n",
    "pprint(metr.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd272a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask rcnn model\n",
    "num_classes = 498\n",
    "\n",
    "model_ft = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model_ft.roi_heads.box_predictor.cls_score.in_features\n",
    "model_ft.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model_ft.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model_ft.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "model_ft.to(DEVICE)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.roi_heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "#all without backbone 19792571\n",
    "\n",
    "\n",
    "print('my', sum(p.numel() for p in model_ft.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32048271",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FoodDataset(TRAIN_IMAGES_PATH, TRAIN_LABELS)\n",
    "val_ds = FoodDataset(VAL_IMAGES_PATH, VAL_LABELS)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True, num_workers=6,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, batch_size=2, shuffle=True, num_workers=6,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0005, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"food\", entity=\"alarnti\")\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 16\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_score = 1e10\n",
    "num_epochs = 100\n",
    "model_ft.cpu()\n",
    "for epoch in range(num_epochs):\n",
    "    model_ft.train()\n",
    "    for i_iter, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        print(images[0].shape)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model_ft(images, targets)\n",
    "    \n",
    "        losses_detached = {key: l.cpu().detach().numpy() for key, l in loss_dict.items()}\n",
    "\n",
    "        loss_mask = losses_detached['loss_mask']\n",
    "        loss_objectness = losses_detached['loss_objectness']\n",
    "        loss_rpn_box_reg = losses_detached['loss_rpn_box_reg']\n",
    "        loss_classifier = losses_detached['loss_classifier']\n",
    "        loss_box_reg = losses_detached['loss_box_reg']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "#         wandb.log({\n",
    "#                     \"loss_mask\": loss_mask,\n",
    "#                     \"loss_objectness\": loss_objectness,\n",
    "#                     \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "#                     \"loss_classifier\": loss_classifier,\n",
    "#                     \"loss_box_reg\": loss_box_reg,\n",
    "#                     \"all_losses\": losses.cpu().detach().numpy()})\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_val_loss = 0\n",
    "    for i_iter, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model_ft(images, targets)\n",
    "        losses_detached = {key: l.cpu().detach().numpy() for key, l in loss_dict.items()}\n",
    "\n",
    "        loss_mask = losses_detached['loss_mask']\n",
    "        loss_objectness = losses_detached['loss_objectness']\n",
    "        loss_rpn_box_reg = losses_detached['loss_rpn_box_reg']\n",
    "        loss_classifier = losses_detached['loss_classifier']\n",
    "        loss_box_reg = losses_detached['loss_box_reg']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        mean_val_loss += losses\n",
    "        \n",
    "#         wandb.log({\n",
    "#                     \"loss_mask_val\": loss_mask,\n",
    "#                     \"loss_objectness_val\": loss_objectness,\n",
    "#                     \"loss_rpn_box_reg_val\": loss_rpn_box_reg,\n",
    "#                     \"loss_classifier_val\": loss_classifier,\n",
    "#                     \"loss_box_reg_val\": loss_box_reg,\n",
    "#                     \"all_losses_val\": losses.cpu().detach().numpy()})\n",
    "    \n",
    "    mean_val_loss /= len(val_loader)\n",
    "        \n",
    "#     wandb.log({'mean_val_loss', mean_val_loss})\n",
    "    lr_scheduler.step(mean_val_loss)\n",
    "    \n",
    "    if mean_val_loss < val_score:\n",
    "        torch.save(model_ft.state_dict(), \n",
    "                   MODEL_SAVE_PATH + 'maskrcnn_' + epoch + '_' + 'val_' + str(mean_val_loss))\n",
    "        val_score = mean_val_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9bc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_validation(model, val_loader):\n",
    "    mean_val_loss = 0\n",
    "    metr = MeanAveragePrecision(\n",
    "                    box_format='xyxy',\n",
    "                    iou_thresholds=None,\n",
    "                    rec_thresholds=[1, 10, 100],\n",
    "                    class_metrics=False,\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_iter, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "            model.train()\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses_detached = {key: l.cpu().detach().numpy() for key, l in loss_dict.items()}\n",
    "\n",
    "            loss_mask = losses_detached['loss_mask']\n",
    "            loss_objectness = losses_detached['loss_objectness']\n",
    "            loss_rpn_box_reg = losses_detached['loss_rpn_box_reg']\n",
    "            loss_classifier = losses_detached['loss_classifier']\n",
    "            loss_box_reg = losses_detached['loss_box_reg']\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            mean_val_loss += losses\n",
    "            \n",
    "            pprint(losses_detached)\n",
    "            \n",
    "            model.eval()\n",
    "            res = model(images)\n",
    "            metr.update(res, targets)\n",
    "            \n",
    "            pprint(res)\n",
    "            \n",
    "            break\n",
    "            if i_iter > 100:\n",
    "                break\n",
    "            \n",
    "    \n",
    "    mean_val_loss /= len(val_loader)\n",
    "    \n",
    "    pprint(metr.compute())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.load_state_dict(torch.load(\"maskrcnn_9_val_tensor(0.7673, device='cuda_0')\"))\n",
    "model_ft.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do_validation(model_ft, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_validation(model_ft, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.cpu().eval()\n",
    "\n",
    "raw_val = [torch_ds[i][0] for i in range(0,10)]\n",
    "trgt = [torch_ds[i][1] for i in range(0,10)]\n",
    "im_val = [torch.mul(255, raw_val[i][0]) for i in range(0,10)]\n",
    "\n",
    "res = model_ft(raw_val)\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4abd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
