{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781571ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import typing\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "import errno\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import FoodDataset\n",
    "from src.vis import read_image, show_image_coco\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00571cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "TRAIN_IMAGES_PATH = 'data/public_training_set_release_2.0/images/'\n",
    "TRAIN_LABELS = 'data/public_training_set_release_2.0/annotations.json'\n",
    "\n",
    "VAL_IMAGES_PATH = 'data/public_validation_set_2.0/images/'\n",
    "VAL_LABELS = 'data/public_validation_set_2.0/annotations.json'\n",
    "\n",
    "# MODEL_SAVE_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = COCO(TRAIN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30708b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = labels.getImgIds()\n",
    "#184135\n",
    "labels.imgToAnns[img_ids[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels.getCatIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52115149",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_coco(img_ids[1], TRAIN_IMAGES_PATH, labels, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40972650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model_ft.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac604b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model_ft.transform(torch.unsqueeze(train_ds[1000][0], dim=0))[0]\n",
    "z.image_sizes# train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[1000][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grcnn = torchvision.models.detection.transform.GeneralizedRCNNTransform(min_size=700, max_size=700, image_mean=[0.485], image_std=[0.229])\n",
    "model_ft.transform = grcnn\n",
    "\n",
    "for el in train_ds:\n",
    "    z = model_ft.transform(torch.unsqueeze(el[0], dim=0))[0]\n",
    "    print(z.image_sizes)# train_ds[0][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2846eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.cpu().eval()\n",
    "labels.loadImgs(img_ids[0])\n",
    "\n",
    "raw_val = [train_ds[i] for i in range(0,10)]\n",
    "trgt = [raw_val[i][1] for i in range(0,10)]\n",
    "im_val = [torch.mul(255, raw_val[i][0]) for i in range(0,10)]\n",
    "# im_val1 = [torch.from_numpy(im_val).float()]\n",
    "res = model_ft(im_val)\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metr = MeanAveragePrecision(\n",
    "                box_format='xyxy',\n",
    "                iou_thresholds=None,\n",
    "                rec_thresholds=[1, 10, 100],\n",
    "                class_metrics=False,\n",
    "                )\n",
    "\n",
    "metr.update(res, trgt)\n",
    "pprint(metr.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd272a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask rcnn model\n",
    "num_classes = 498\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model_ft = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model_ft.roi_heads.box_predictor.cls_score.in_features\n",
    "model_ft.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model_ft.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model_ft.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "model_ft.to(device)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32048271",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FoodDataset(TRAIN_IMAGES_PATH, TRAIN_LABELS)\n",
    "val_ds = FoodDataset(VAL_IMAGES_PATH, VAL_LABELS)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True, num_workers=6,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, batch_size=2, shuffle=True, num_workers=6,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0005, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"food\", entity=\"alarnti\")\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 16\n",
    "}\n",
    "\n",
    "val_score = 1e10\n",
    "num_epochs = 100\n",
    "model_ft.cuda()\n",
    "for epoch in range(num_epochs):\n",
    "    model_ft.train()\n",
    "    for i_iter, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        print(images[0].shape)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model_ft(images, targets)\n",
    "    \n",
    "        losses_detached = {key: l.cpu().detach().numpy() for key, l in loss_dict.items()}\n",
    "\n",
    "        loss_mask = losses_detached['loss_mask']\n",
    "        loss_objectness = losses_detached['loss_objectness']\n",
    "        loss_rpn_box_reg = losses_detached['loss_rpn_box_reg']\n",
    "        loss_classifier = losses_detached['loss_classifier']\n",
    "        loss_box_reg = losses_detached['loss_box_reg']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        wandb.log({\n",
    "                    \"loss_mask\": loss_mask,\n",
    "                    \"loss_objectness\": loss_objectness,\n",
    "                    \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "                    \"loss_classifier\": loss_classifier,\n",
    "                    \"loss_box_reg\": loss_box_reg,\n",
    "                    \"all_losses\": losses.cpu().detach().numpy()})\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    \n",
    "    model_ft.eval()\n",
    "    mean_val_loss = 0\n",
    "    for i_iter, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model_ft(images, targets)\n",
    "        losses_detached = {key: l.cpu().detach().numpy() for key, l in loss_dict.items()}\n",
    "\n",
    "        loss_mask = losses_detached['loss_mask']\n",
    "        loss_objectness = losses_detached['loss_objectness']\n",
    "        loss_rpn_box_reg = losses_detached['loss_rpn_box_reg']\n",
    "        loss_classifier = losses_detached['loss_classifier']\n",
    "        loss_box_reg = losses_detached['loss_box_reg']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        mean_val_loss += losses\n",
    "        \n",
    "        wandb.log({\n",
    "                    \"loss_mask_val\": loss_mask,\n",
    "                    \"loss_objectness_val\": loss_objectness,\n",
    "                    \"loss_rpn_box_reg_val\": loss_rpn_box_reg,\n",
    "                    \"loss_classifier_val\": loss_classifier,\n",
    "                    \"loss_box_reg_val\": loss_box_reg,\n",
    "                    \"all_losses_val\": losses.cpu().detach().numpy()})\n",
    "    \n",
    "    mean_val_loss /= len(val_loader)\n",
    "        \n",
    "    wandb.log({'mean_val_loss', mean_val_loss})\n",
    "    lr_scheduler.step(mean_val_loss)\n",
    "    \n",
    "    if mean_val_loss < val_score:\n",
    "        torch.save(model_ft.state_dict(), \n",
    "                   MODEL_SAVE_PATH + 'maskrcnn_' + epoch + '_' + 'val_' + str(mean_val_loss))\n",
    "        val_score = mean_val_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9bc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"food\", entity=\"alarnti\", mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8db51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2adf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
